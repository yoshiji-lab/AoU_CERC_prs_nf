nextflow.enable.dsl = 2

params {
  /*
   * =========================
   * REQUIRED: YOUR BUCKET
   * =========================
   */
  bucket = 'gs://fc-secure-c2a3ec74-f528-4692-a6c3-00d9f1946f4c'

  /*
   * =========================
   * INPUTS
   * =========================
   */

  // Weight files (one per trait)
  weights_glob = "workspaces/allofusgwasonmetabolictraits/urvashi_analysis/inputs/test"

  // PGEN dataset on GCS (must contain snpqc.chr1.pgen/.pvar/.psam etc)
  // NOTE: plink2 cannot read gs:// directly -> Nextflow must stage these files
  pfile_dir  = "${params.bucket}/GWAS_core/acaf_snp_qc_oct052025"
  pfile_pref = "snpqc.chr{chrom}"

  chroms = (21..22)

  /*
   * =========================
   * OUTPUTS / CACHE SAFETY
   * =========================
   */

  // Final results go here
  outdir  = "${params.bucket}/urvashi_analysis/01.ukb_v20/prs_out_idp_adiposity_test01"

  // Nextflow cache + task sandboxes live here (this is what makes -resume work)
  workdir = "${params.bucket}/nf_work/prs_nf"

  /*
   * =========================
   * BEHAVIOR
   * =========================
   */
  allow_strand_flip   = false
  write_list_variants = false

  /*
   * =========================
   * RESOURCES (cost-aware)
   * =========================
   */
  threads_small   = 4
  memory_small    = '8 GB'
  disk_small      = '120 GB'

  threads_medium  = 6
  memory_medium   = '12 GB'
  disk_medium     = '180 GB'

  threads_large   = 8
  memory_large    = '16 GB'
  disk_large      = '250 GB'

  // Concurrency control
  max_forks = 50

  /*
   * =========================
   * TOOLS / CONTAINER
   * =========================
   */

  // Artifact Registry repo root (AoU-style)
  artifact_registry_repo = System.getenv('ARTIFACT_REGISTRY_DOCKER_REPO') ?: 'us-docker.pkg.dev/YOUR_PROJECT/YOUR_REPO'

  /*
   * IMPORTANT:
   * Your pipeline needs python3 + pandas + plink2 in the same container.
   * Replace this with your built image.
   */
  container  = "${params.artifact_registry_repo}/prs/plink2-py:latest"

  // The executable inside the container
  plink2_bin = 'plink2'
}

workDir = params.workdir

google {
  region = 'us-central1'
}

process {
  executor = 'google-batch'
  container = params.container

  // Keep bash strict mode everywhere
  shell = ['/bin/bash', '-euo', 'pipefail']

  // Retry a little, then ignore failures at the *task* level
  errorStrategy = { task.attempt <= 2 ? 'retry' : 'ignore' }
  maxRetries = 2
  maxErrors  = -1

  // Default resources (overridden below)
  cpus   = params.threads_medium
  memory = params.memory_medium
  disk   = params.disk_medium

  withName: SCORE_TRAIT_CHR {
    maxForks = params.max_forks

    cpus = {
      def chr = (task.ext.chrom as Integer)
      if (chr >= 19) return params.threads_small
      else if (chr >= 11) return params.threads_medium
      else return params.threads_large
    }

    memory = {
      def chr = (task.ext.chrom as Integer)
      if (chr >= 19) return params.memory_small
      else if (chr >= 11) return params.memory_medium
      else return params.memory_large
    }

    disk = {
      def chr = (task.ext.chrom as Integer)
      if (chr >= 19) return params.disk_small
      else if (chr >= 11) return params.disk_medium
      else return params.disk_large
    }
  }

  withName: MERGE_TRAIT {
    cpus = 4
    memory = '16 GB'
    disk = '120 GB'
    maxForks = 10
  }

  withName: BUILD_MATRIX {
    cpus = 4
    memory = '32 GB'
    disk = '200 GB'
  }
}

/*
 * =========================
 * OPTIONAL: NEXTFLOW REPORTS
 * =========================
 *
 * These are generated by the *controller* (your notebook VM/container).
 * If you want them to survive VM death, ALSO run nextflow with:
 *   -log ${params.outdir}/nextflow.log
 *
 * (Nextflow supports changing log file location via the -log flag.) :contentReference[oaicite:0]{index=0}
 */
trace {
  enabled = true
  file = "${params.outdir}/pipeline_trace.txt"
  fields = 'task_id,hash,native_id,process,tag,name,status,exit,submit,start,complete,duration,realtime,cpus,memory,disk,attempt'
}

report {
  enabled = true
  file = "${params.outdir}/pipeline_report.html"
}

timeline {
  enabled = true
  file = "${params.outdir}/pipeline_timeline.html"
}

dag {
  enabled = true
  file = "${params.outdir}/pipeline_dag.svg"
}

profiles {

  // ============================
  // LOCAL mode (runs on the VM)
  // ============================
  local {

    // IMPORTANT: local executor needs local workDir
    process.executor = 'local'

    // Use local POSIX paths (relative to prs_nf/)
    params.pfile_dir    = '../urvashi_analysis/pgen_dir'
    params.weights_glob = '../urvashi_analysis/inputs/test/*.tsv.gz'

    // Keep outputs local (fast). You can gsutil rsync afterwards.
    params.outdir  = '../urvashi_analysis/prs_out_idp_adiposity_LOCAL'
    params.workdir = '../urvashi_analysis/nf_work/prs_nf_LOCAL'

    // Use your VM-installed tools (no container needed)
    process.container = null
    params.container  = null
    params.plink2_bin = 'plink2'

    // Local optimization: do NOT copy huge .pgen files into each task dir
    process.stageInMode = 'symlink'

    // Control parallelism on ONE VM
    params.max_forks = 2

    process {
      // Cap concurrency + CPU usage
      maxForks = params.max_forks
      cpus     = 8
      memory   = '16 GB'
    }

    // Avoid “Report file already exists” errors
    report.overwrite   = true
    timeline.overwrite = true
    dag.overwrite      = true
    trace.overwrite    = true
  }
}

